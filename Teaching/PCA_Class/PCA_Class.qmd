---
title: "PCA_Class"
editor: source
freeze: auto
execute:
  warning: false
lang: fr
---

## Les mathématiques de l'ACP
Source : <https://medium.com/intuition/mathematical-understanding-of-principal-component-analysis-6c761004c2f8>

```{r}
#| include: false

library(tidyverse)
library(plotly)
library(knitr)

set.seed(248655)
```

Considérons une matrice $x$ de taille $n \times p$ représentant $n$ individus et leurs $p$ paramètres associés :

```{r}
#| echo: false
#| label: tbl-values
#| tbl-cap: "Extrait de traits de 100 individus"

x <- rnorm(100, mean = 5, sd = .5)
y <- 2*x + rnorm(100)
data <- data.frame(x, y)
rm(x, y)

pretty_data <- data |> 
  mutate(Individus = row_number()) |> 
  rename("Trait 1" = x, "Trait 2" = y) |> 
  select(c(Individus, "Trait 1", "Trait 2"))

kable(head(pretty_data))
```

On peut également considérer qu'il s'agit de $n$ réalisations de $p$ variables aléatoires $X_{1}, \dots, X_{p}$.\
Ainsi, une ligne de $x$ correspond à un individu que l'on représente par le vecteur ligne $x_{i}$ avec ses $p$ paramètres :

$$
x_{i} = \begin{bmatrix} x_{1} \dots x_{p}\end{bmatrix}_{i} \text{ et } x_{i}^T = \begin{bmatrix}
      x_{1}\\
      \vdots \\
      x_{p}
    \end{bmatrix}_{i} \text{ sa transposée.}
$$
On peut représenter le jeu de données, la matrice $x$, par $n$ points dans un espace à $p$ dimensions comme dans notre exemple :

```{r}
#| echo: false
#| label: fig-traits
#| fig-cap: Biplot des deux traits d'intérêt

ggplot(data) + 
  geom_point(aes(x = x, y = y)) + 
  labs(x = "Trait 1", y = "Trait 2") +
  theme_minimal()
```

Pour faciliter les calculs, on centre et réduit les $X_{p}$ variables aléatoires :

```{r}
#| echo: false

data <- data |>
  mutate(
    scale_x = scale(x),
    scale_y = scale(y)
  )

angle <- data.frame(theta = round(seq.int(0, pi/2, length.out = 22)[-c(1, 22)], digits = 2))

data_anim <- cbind(data, "angle" = angle$theta[1])

for (i in 2:dim(angle)[1]) {
  data_anim <- rbind(data_anim, cbind(data, "angle" = angle$theta[i]))
}

data_anim <- data_anim |> 
  mutate(
    x_proj = (scale_x*cos(angle) + scale_y*sin(angle))*cos(angle),
    y_proj = (scale_x*cos(angle) + scale_y*sin(angle))*sin(angle)
  )
```

```{r}
#| echo: false
#| label: fig-traitssclaed
#| fig-cap: Biplot des deux traits d'intérêt centrés et réduits

ggplot(data = data_anim) +
  geom_point(aes(x = scale_x, y = scale_y)) + 
  coord_fixed(ratio = 1) +
  theme_minimal()
```

On cherche à trouver un axe représenté par un vecteur $w$ de l'espace des paramètres, donc à $p$ dimensions, tel qu'il maximise la variance du jeu de données, ou nuage de point, projetée sur celui-ci.

C'est aussi équivalent à minimiser la somme du carré des résidus :

```{r}
#| echo: false
#| label: fig-VarMSE
#| fig-cap: Equivalentre entre MSE et Variance

variance <- ggplot(data = data_anim) +
  geom_segment(
    aes(frame = angle, x = scale_x, xend = x_proj, y = scale_y, yend = y_proj),
    linetype = 2,
    colour   = "grey"
  ) +
  geom_abline(aes(frame = angle, slope = tan(angle), intercept = 0), colour = "grey") +
  geom_point(aes(frame = angle, x = scale_x, y = scale_y)) + 
  geom_point(aes(frame = angle, x = x_proj, y = y_proj), colour = "red") +
  coord_fixed(ratio = 1) +
  theme_minimal()

MSE <- ggplot(data = data_anim) +
  geom_segment(
    aes(frame = angle, x = scale_x, xend = x_proj, y = scale_y, yend = y_proj),
    linetype = 1,
    colour   = "red"
  ) +
  geom_abline(aes(frame = angle, slope = tan(angle), intercept = 0), colour = "grey") +
  geom_point(aes(frame = angle, x = x_proj, y = y_proj), colour = "grey") +
  geom_point(aes(frame = angle, x = scale_x, y = scale_y)) + 
  coord_fixed(ratio = 1) +
  theme_minimal()

subplot(ggplotly(variance), ggplotly(MSE))
```

Posons le problème mathématiquement :

![Projection de $x_{i}$ sur $w$](figures/angle.png){#fig-proj}

Soit $x_{i}$ une composante de $x$ et $w$ un vecteur unitaire (donc $\|w\| = 1$ par définition). On a :

$$
\text{cos}(\theta) = \frac{x_{i} \cdot w}{\|x_{i}\|\|w\|}
$$

Tel que la projection orthogonale de $x_{i}$ sur $w$ est de norme :

$$
\|x_{i}\| \text{cos}(\theta) = \|x_{i}\|  \frac{x_{i} \cdot w}{\|x_{i}\|\|w\|} = x_{i} \cdot w
$$

On a donc le résidu associé :

\begin{align}

\| x_{i} - (x_{i} \cdot w)w \|^2 &= (x_{i} - (x_{i} \cdot w)w) \cdot  (x_{i} - (x_{i} \cdot w)w) \\
                                 &= \|x_{i}\|^2 - 2(w \cdot x_{i})^2 + (x_{i} \cdot w)^2w \cdot w \\
                                 &= x_{i} \cdot x_{i} - (w \cdot x_{i})^2

\end{align}

On a donc la somme des rédisus ou Mean Squared Error (MSE)

\begin{align}

\text{MSE}(w) &= \frac{1}{n} \sum_{i = 1}^n(\|x_{i}\|^2 - (w \cdot x_{i})^2) \\
&= \frac{1}{n} \sum_{i = 1}^n\|x_{i}\|^2 - \frac{1}{n} \sum_{i = 1}^n(w \cdot x_{i})^2 \\

\end{align}

On veut trouver $w$ tel que $\text{MSE}(w)$ soit minimale. Le premier terme est constant en fonction de $w$, il reste donc à maximiser le second terme (il y a un $-$ !). On remarque que le second terme peut s'écrire $\frac{1}{n} \sum_{i = 1}^n(w \cdot x_{i})^2 = E\left[(w \cdot x_{i})^2\right]$ or on a la relation $\text{Var}(X) = E[X^2] - E[X]^2$ d'où l'égalite :

\begin{align}

\frac{1}{n} \sum_{i = 1}^n(w \cdot x_{i})^2 &= (\frac{1}{n} \sum_{i = 1}^n w \cdot x_{i})^2 + \text{Var}(w \cdot x_{i}) \\
&= \text{Var}(w \cdot x_{i})

\end{align}

Car $w \cdot x_{i} = x_{i}$ puisque $w$ est unitaire et les $X_{p}$ sont centrées donc de moyennes nulles donc $\frac{1}{n} \sum_{i = 1}^n x_{i} = 0$.

Ainsi on a :

$$
\text{MSE}(w) = \frac{1}{n}\sum_{i = 1}^n \|x_{i}\|^2 - \text{Var}(w \cdot x_{i})
$$

Minimiser MSE revient donc à maximiser la variance des projections des $x_{i}$ mais comment faire ?

En rappelant :

$$
\text{Var}(x_{i}) = \frac{1}{p}x_{i}^Tx_{i} = \frac{1}{p} \sum_{j = 1}^px_{j, (i)}^2
$$

On obtient :

\begin{align}

\sigma_{w}^2 = \text{Var}(w \cdot x_{i}) &= \frac{1}{n} \sum_{i = 1}^n(w \cdot x_{i})^2 \\
&= \frac{1}{n}(xw)^T(xw) \\
&= \frac{1}{n}w^Tx^Txw \\
\sigma_{w}^2 &= w^Tcov_{x^T,x}w

\end{align}

avec $cov_{x^T,x}$ la matrice de variance-covariance des $X_{p}$ variables aléatoires correspondants à nos paramètres :
\begin{align}

cov_{x^T,x} = \text{Cov}\left[x^T,x\right] &= E\left[(x - \mu_{x})^T(x - \mu_{x})\right] \\
&= E\left[x^Tx\right] \text{ en centrant les } X_{p}\\
\text{En développant :} \\
cov_{x^T,x} &= \begin{bmatrix} \text{Var}(X_{1}) & \dots & \text{Cov}(X_{1}, X_{p}) \\
                              \vdots & \ddots & \vdots \\
                              \text{Cov}(X_{p}, X_{1}) & \dots & \text{Var}(X_{p}) \end{bmatrix}

\end{align}

*N.B* : Attention à la dimensionnalité, $\frac{1}{n}x^Tx = cov_{x^T,x}$ est une **matrice** de covariance (de dimension $p \times p$) alors que $\frac{1}{n}xx^T = \frac{1}{n} \sum_{i = 1}^nx_{i}^2$ est un nombre (de dimension $1 \times 1$).

Ainsi, minimiser $\text{MSE}(w)$ revient à trouver $w$, vecteur unitaire, tel que $\sigma_{w}^2$ soit minimisé.\
Pour minimiser $\sigma_{w}^2$, on peut utiliser l'opérateur Lagrangien avec la contrainte que $w$ doit être un vecteur unitaire, c'est à dire que $\|w\| = w^Tw = 1$ comme suit :

\begin{align}

\mathcal{L}(w, \lambda) &\equiv \sigma_{w}^2 - \lambda(w^Tw - 1) \\
\frac{\partial\mathcal{L}}{\partial \lambda} &= (w^Tw - 1) \\
\frac{\partial\mathcal{L}}{\partial w} &= 2cov_{X^T,X}w - 2\lambda w

\end{align}

On a donc à l'optimum, en posant les dérivées partielles nulles, le système d'équations :

\begin{align}

w^Tw&= 1 \\
cov_{x^T,x}w &= \lambda w

\end{align}

Ce qui revient à trouver les valeurs et vecteurs propres de la matrice de covariance de X ! De plus le vecteur propre associé à la valeur propre la plus élevée maximisera la covariance.

Et ça nous arrange bien car la matrice de covariance a des propriétés intéressantes : c'est une matrice carrée, positive, symétrique et inversible. Donc il existe une décomposition en valeurs propres de cette matrice tel que $cov_{x^T,x}$ se factorise sous la forme :

$$
cov_{x^T,x} = x^Tx = W \Lambda W^{-1}
$$
Avec $W$, de taille $p \times p$, la matrice diagonlae des vecteurs propres et $\Lambda$, de taille $p \times p$, la matrice des valeurs propres de $cov_{x^T,x}$.\
Les vecteurs propres ainsi trouvés sont tous unitaires et orthogonaux et forment une base de l'espace à $p$ dimensions de départ.

On a alors les coordonnées de notre nuage de points initial dans le nouveau repère défini par $W$ qui sont données par :

$$
T = xW
$$

On appelle T la matrice des poids.

En pratique, la diagonalisation de $cov_{x^T,x}$ est gourmande en calculs car elle nécessite le passer par l'intermédiaire $x^T$. Une "astuce" informatique consiste à réaliser une décomposition en valeurs singulières de $x$ tel que :

$$
x = U \Sigma V^T
$$

avec U une matrice unitaire (càd $UU^T = I$) de taille $n \times n$, V une matrice unitaire (càd $UU^T = I$) de taille $p \times p$ et $\Sigma$ une matrice diagonale rectangulaire de taille $n \times p$ dont les coefficients diagonaux sont les $\sqrt{\lambda_{i}}$ non nuls.\
On a alors la relation :

$$
x^Tx = V (\Sigma^T\Sigma) V^T =  W \Lambda W^{-1}
$$

On retrouve une décomposition en valeurs propres. En décomposant $x$ en valeurs singulières, on s'évite de passer par le calcul de son inverse.

![Interprétation graphique de la Décomposition en Valeurs Singulières](figures/Singular-Value-Decomposition.png){#fig-svd}

## Example simple à trois dimensions

```{r}
library(FactoMineR)
```

```{r}
#| echo: false
#| label: tbl-iris
#| tbl-cap: "Extrait des valeurs des données Iris"

data("iris")

iris_3D <- iris |> 
  select(c(Species, Sepal.Width, Petal.Width, Petal.Length))

kable(head(iris_3D))
```

```{r}
#| echo: false
#| label: fig-iris
#| fig-cap: Plots des valeurs de Iris

iris |> plot_ly(
  x      = ~Sepal.Width,
  y      = ~Petal.Width,
  z      = ~Petal.Length,
  color  = ~Species,
  colors = c('#BF382A', '#0C4B8E', "green")
) |> 
  add_markers() |> 
  layout(
    scene = list(
      xaxis      = list(title = "Sepal Width"),
      yaxis      = list(title = "Petal Width"),
      zaxis      = list(title = "Petal Length"),
      aspectmode = 'data'
    )
  )
```

```{r}
#| echo: false
#| label: fig-irisscaled
#| fig-cap: Plots des valeurs de Iris centrées et réduites

iris |> 
  mutate(
    Sepal.Length = scale(Sepal.Length),
    Sepal.Width  = scale(Sepal.Width),
    Petal.Length = scale(Petal.Length),
    Petal.Width  = scale(Petal.Width)
    ) |> 
  plot_ly(
    x      = ~Sepal.Width,
    y      = ~Petal.Width,
    z      = ~Petal.Length,
    color  = ~Species,
    colors = c('#BF382A', '#0C4B8E', "green")
  ) |> 
  add_markers() |> 
  layout(
    scene = list(
      xaxis      = list(title = "Sepal Width"),
      yaxis      = list(title = "Petal Width"),
      zaxis      = list(title = "Petal Length"),
      aspectmode = 'data'
    )
  )
```

```{r}
#| echo: false
#| label: fig-irispca
#| fig-cap: Plots des valeurs de l'ACP de Iris

iris |>
  select(-c(Sepal.Length, Species)) |> 
  PCA(graph = FALSE) |> 
  _$ind |> 
  _$coord |>
  as_tibble() |>
  mutate(Species = iris$Species) |> 
  plot_ly(
    x      = ~Dim.1,
    y      = ~Dim.2,
    z      = ~Dim.3,
    color  = ~Species,
    colors = c('#BF382A', '#0C4B8E', "green")
  ) |> 
  add_markers() |> 
  layout(
    scene = list(
      domain     = list(x = c(-3, 3), y = c(-3, 3), z = c(-3, 3)),
      xaxis      = list(title = "PC 1"),
      yaxis      = list(title = "PC 2"),
      zaxis      = list(title = "PC 3"),
      aspectmode = 'data'
    )
  )
```

## Et en pratique ?

```{r}
library(factoextra)
library(patchwork)
```

```{r}
iris_pca <- iris |>
  select(-c(Species)) |>
  PCA(
    scale.unit = TRUE,
    ncp        = 5,
    graph      = FALSE
  )

iris_pca
```

```{r}
iris |>
  select(-c(Species)) |>
  prcomp(scale. = TRUE)
```

[Aide](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp) de la fonction `prcomp()`.

On commence par deux types de graphiques : Un Screeplot et un cercle des corrélations.

```{r}
#| echo: false
#| label: fig-screecirc
#| fig-cap: Screeplot et cercle des corrélations pour Iris

fviz_eig(iris_pca, addlabels = TRUE) + fviz_pca_var(iris_pca, col.var = "black")
```

La fonction `fviz_pca_var()` permet de colorer les vecteurs des variables par le carré de leur cosinus avec les vecteurs propres :

```{r}
iris_pca |>
  fviz_pca_var(
    col.var       = "cos2",
    gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
    repel         = TRUE # Avoid text overlapping
  )
```

Une autre manière de visualiser ces contributions :

```{r}
library(corrplot)

var <- get_pca_var(iris_pca)
corrplot(var$cos2, is.corr = FALSE)

fviz_cos2(iris_pca, choice = "var", axes = 1:2)
fviz_contrib(iris_pca,
             choice = "var",
             axes  = 1,
             top   = 10) + 
  fviz_contrib(iris_pca,
               choice = "var",
               axes   = 2,
               top  = 10)
```

On peut maintenant regarder ce qu'il se passe du côté des individus :

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(1, 2),
  geom.ind   = "point",
  mean.point = FALSE
)
```

Il semble y avoir plusieurs groupes, ajoutons une variable catégorielle pour aider à la visualisation :

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(1, 2),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)
```

Les axes 1 et 2 de l'ACP nous permettent de distinguer clairement deux groupes qui correspondent bien à des espèces différentes.

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(2, 3),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)

iris_pca |> fviz_pca_ind(
  axes       = c(3, 4),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)
```

Par contre les axes 2 et 3 sont moins informatifs dans ce cas, comme prédis par le screeplot.

## A retenir de l'ACP

*   C'est une analyse exploratoire d'un jeu de données *a priori* corrélées, pas un test statistique. Elle permet de décrire la variance d'un jeu de données

*   Cette analyse permet de réduire la dimensionnalité des données et les résume par des composantes principales

* Ces composantes principales sont orthogonales (*i. e* non corrélés) et participent de manière décroissante à la variance totale du jeu de données

* Le nouvel espace est Euclidien, on peut y mesurer des distances et des angles. Attention cependant à leur interprétation !

## Analyses de la même famille

WORK IN PROGRESS

<!-- Source : <http://factominer.free.fr/factomethods/index_fr.html> -->

<!-- ### Méthodes Classiques -->

<!-- Quand des individus sont décrits par un jeu de variables, plusieurs méthodes sont possibles selon le type de variables considéré (quantitatives ou qualitatives): -->

<!-- #### Ordination sans contraintes -->

<!-- -   Quand les variables sont quantitatives, on peut réaliser une Analyse en Composantes Principales (ACP ou PCA). On a alors des *distances Euclidiennes* entre individus -->

<!-- -     Analyse en *Coordonnées* Principales (ACoP ou PCoA) si on veut utiliser une distance encore différente. C'est la version la plus générale de ces analyses -->

<!-- #### Ordination sous contraintes -->

<!-- -   Quand les individus sont décrits par deux variables qualitatives, on peut construire un tableau de contingence et réaliser une Analyse Factorielle des Correspondances (AFC ou CA). L’une des hypothèses clefs de la PCA postule que les individus sont liés façon linéaire. Quand ce n'est pas le cas on peut avoir des artéfacts, on peut alors utiliser une distance du Chi2 -->

<!-- -   Quand les individus sont décrits par un jeu de variables qualitatives, on peut réaliser une Analyse des Correspondances Multiples (ACM ou MCA). -->

<!-- La fonction HCPC (Classification Hiérarchique sur Composantes principales) permet de réaliser une classification non supervisée des individus. Cette fonction combine les facteurs principaux, la classification hérarchique et le partitionnement pour mieux visualiser et mettre l'accent sur les similarités entre individus. -->

<!-- ### Méthodes Avancées -->

<!-- #### Un groupe d'individus et plusieurs groupes de variables -->

<!-- Quand des individus sont décrits par plusieurs groupes de variables, plusieurs types d'analyses sont proposées: -->

<!-- -   AFM (Analyse Factorielle Multiple), où les variables d'un même groupe peuvent être quantitatives ou qualitatives -->

<!-- -   AFMH (Analyse Factorielle Multiple Hiérarchique), une extension de l'AFM où les variables sont structurées selon une hiérarchie. -->

<!-- -   GPA (Analyse Procustéenne Généralisée), où les variables doivent être quantitatives. -->

<!-- #### Un groupe de variables, plusieurs groupes d'individus -->

<!-- Quand plusieurs groupes d'individus sont décrits par un groupe de variables quantitatives, l'analyse que l'on propose est une extension de l'AFM appelée AFM Duale. -->

<!-- #### Un groupe d'individus, deux types de variables -->

<!-- Quand un groupe d'individus est décrit par un groupe de variables quantitatives et/ou qualitatives, on propose comme analyse un cas particulier de l'AFM appelé Analyse Factorielle de Données Mixtes. -->

<!-- ### Analyses discriminantes -->

