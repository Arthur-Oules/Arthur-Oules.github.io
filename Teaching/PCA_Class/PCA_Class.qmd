---
title: "TP Ecolostats – PCA"
author:
  - name: "Arthur Oulès"
    orcid: 0009-0005-0455-1824
    email: arthur.oules@ens-lyon.fr
    affiliation:
      - name: Laboratoire de Biologie Tissulaire et d'Ingénierie Thérapeutique – UMR 5305
        city: Lyon 7
        country: France
        url: https://lbti.ibcp.fr/?page_id=1826
  - name: "Adapté de [medium.com](https://medium.com/intuition/mathematical-understanding-of-principal-component-analysis-6c761004c2f8)"
description: "Lecture on the mathematical aspects of the PCA and its use in ecology."
categories: Lecture
editor: source
freeze: auto
execute:
  warning: false
lang: fr
---

## Les Mathématiques de l'ACP
### Définitions

```{r}
#| include: false

library(tidyverse)
library(patchwork)
library(plotly)
library(knitr)

set.seed(248655)
```

Considérons une matrice réelle $X \in \mathcal{M}_{n,p}(\mathbb{R})$ de dimension $n \times p$ représentant la mesure de $p \in \mathbb{N}^{*}$ paramètres mesuré chez $n \in \mathbb{N}^{*}$ individus d'une population telle que :

$$
X =  \begin{bmatrix} x_{1, 1} & \dots  & x_{1, p}  \\
                       \vdots & \ddots & \vdots \\
                     x_{n, 1} & \dots  & x_{n, p} \end{bmatrix}
$$

Avec $\forall i \in [\![1, n]\!], \forall j \in [\![1, p]\!], x_{i,j} \in \mathbb{R}$ la valeur de la $i$-ème ligne et $j$-ième colonne qui correspond à la $i$-ème mesure de l'individu $j$.

Ce qui donne en pratique :

```{r}
#| echo: false
#| label: tbl-values
#| tbl-cap: "Extrait de $p = 2$ traits de $n = 50$ individus"

data <- tibble(
  x = rnorm(50, mean = 5, sd = .5),
  y = 2*x + rnorm(50)
)

data |> 
  mutate(Individus = row_number()) |> 
  rename("Trait 1" = x, "Trait 2" = y) |> 
  select(c(Individus, "Trait 1", "Trait 2")) |> 
  head() |> 
  kable()

```

On peut également considérer qu'il s'agit d'un tableau de $n$ réalisations de $p$ variables aléatoires $X_{1}, \dots, X_{p}$ telles que $\forall j \in [\![1,p]\!], X_{j} \sim \mathcal{N}(\mu, \sigma^{2})$. Ainsi, une colonne correspond à $n$ tirages de la variable aléatoire $X_{j \in [\![1,p]\!]}$.\
Une ligne de la matrice $X$ correspond aux mesures d'un individu que l'on représente par le vecteur $\vec{x_{i}} \in \mathbb{R}^{p}$ avec ses $p$ coordonnées, ou paramètres, qui sont une réalisation des $X_{i \in [\![1, p]\!]}$.\
On a l'écriture matricielle du vecteur $\vec{x_{i}}$ :

$$
\vec{x_{i}} = \begin{bmatrix} x_{1}\\
                              \vdots \\
                               x_{p} \end{bmatrix}_{i} \text{ et } \vec{x_{i}}^T = \begin{bmatrix} x_{1} \dots x_{p}\end{bmatrix}_{i} \text{ sa transposée.}
$$
On notera la norme usuelle (ou euclidienne) :

$$
\vec{x_{i}} \cdot \vec{x_{i}} = \|\vec{x_{i}}\|^{2} = \sum_{j = 1}^{p} x_{i,j}^{2} = \begin{bmatrix} x_{1} \dots x_{p}\end{bmatrix}_{i} \times \begin{bmatrix} x_{1}\\
                              \vdots \\
                               x_{p} \end{bmatrix}_{i} = \vec{x_{i}}^{T} \times \vec{x_{i}}
$$

On peut représenter le jeu de données, la matrice $X$, par $n$ points dans un espace à $p$ dimensions comme dans notre exemple :

```{r}
#| echo: false
#| label: fig-traits
#| fig-cap: Biplot des deux traits d'intérêt

ggplot(data) + 
  geom_point(aes(x = x, y = y)) + 
  labs(x = "Trait 1", y = "Trait 2") +
  theme_minimal()
```

Pour faciliter les calculs, on centre et réduit (*scale* en anglais) les $X_{p}$ variables aléatoires :

::: {.callout-note}
Pour center-réduire, on pose $Z = \frac{X - \mu_{X}}{\sigma_{x}}$ ainsi $Z \sim \mathcal{N}(0, 1)$. Par comodité de notation, on confondra Z et X dans la suite.
:::

```{r}
#| echo: false
#| label: fig-traitssclaed
#| fig-cap: Biplot des deux traits d'intérêt centrés et réduits

data |>
  mutate(
    scale_x = scale(x)[ ,1],
    scale_y = scale(y)[, 1]
  ) |> 
  ggplot() +
  geom_point(aes(x = scale_x, y = scale_y)) + 
  coord_fixed(ratio = 1) +
  labs(
    x = "Trait 1 centré-réduit",
    y = "Trait 2 centré-réduit"
  ) +
  theme_minimal()
```

### L'ACP : Une régression linéaire

On cherche à trouver un axe représenté par un vecteur unitaire $\vec{w} \in \mathbb{R}^{p}, \|\vec{w}\| = 1$ (et sa notation matricielle en vecteur colonne) de l'espace des paramètres, donc à $p$ dimensions, tel qu'il maximise la variance du jeu de données, ou nuage de point, projetée sur celui-ci.

Graphiquement, cela équivaut à minimiser la somme du carré des résidus :

```{r}
#| echo: false

data <- tibble(
  x = rnorm(50, mean = 5, sd = .5),
  y = 2*x + rnorm(50)
) |>
  mutate(
    scale_x = scale(x)[ ,1],
    scale_y = scale(y)[, 1]
  )

angle <- data.frame(theta = seq(0, pi/2, by = pi/36))

# angle <- data.frame(theta = c(0, pi/16, pi/8, 3*pi/16, pi/4, 5*pi/16, 3*pi/8, 7*pi/16, .995*pi/2))

data_anim <- cbind(data, "angle" = angle$theta[1])

for (i in 2:dim(angle)[1]) {
  data_anim <- rbind(data_anim, cbind(data, "angle" = angle$theta[i]))
}

data_anim <- data_anim |> 
  tibble() |> 
  mutate(θ = round(angle*180/pi)) |> 
  mutate(norm = scale_x*cos(angle) + scale_y*sin(angle)) |> 
  mutate(
    x_proj = norm*cos(angle),
    y_proj = norm*sin(angle)
  )

# data_res <- data_anim |> 
#   group_by(θ) |> 
#   mutate(sigma2 = sd(norm)) |> 
#   mutate(MSE = mean(scale_x**2 + scale_y**2) - mean(norm**2)) |>
#   distinct(θ, .keep_all = TRUE) |>
#   select(θ, sigma2, MSE)
#     
```

```{r}
#| echo: false
#| label: fig-VarMSE
#| fig-cap: Equivalence entre MSE et Variance sur l'exemple de notre jeu de données

variance <- data_anim |>
  ggplot() +
  geom_segment(
    aes(
      frame = θ,
      x = scale_x, xend = x_proj,
      y = scale_y, yend = y_proj
    ),
    linetype = 2,
    colour   = "grey"
  ) +
  geom_abline(aes(frame = θ, slope = tan(angle), intercept = 0), colour = "grey") +
  geom_point(aes(frame = θ, x = scale_x, y = scale_y)) + 
  geom_point(aes(frame = θ, x = x_proj, y = y_proj), colour = "red") +
  # geom_text(
  #   data = data_res,
  #   aes(
  #     frame = θ,
  #     x = .8, y = -2.5,
  #     size  = .5,
  #     label = paste("σ<sup>2</sup><sub>θ</sub> = ", as.character(round(sigma2, digits = 3)))
  #   )
  # ) +
  coord_fixed(ratio = 1) +
  labs(
    x = "Trait 1 centré-réduit",
    y = "Trait 2 centré-réduit"
  ) +
  theme_minimal()

# ggplotly(variance) |> animation_opts(frame = 500, mode = "next")

MSE <- ggplot(data = data_anim) +
  geom_segment(
    aes(frame = θ, x = scale_x, xend = x_proj, y = scale_y, yend = y_proj),
    linetype = 1,
    colour   = "red"
  ) +
  geom_abline(aes(frame = θ, slope = tan(angle), intercept = 0), colour = "grey") +
  geom_point(aes(frame = θ, x = x_proj, y = y_proj), colour = "grey") +
  geom_point(aes(frame = θ, x = scale_x, y = scale_y)) +
  # geom_text(
  #   aes(
  #     frame = θ,
  #     x = .8, y = -2.5,
  #     size  = .5,
  #     label = paste0("MSE(θ) = ", as.character(round(MSE, digits = 3)))
  #   )
  # ) +
  coord_fixed(ratio = 1) +
  labs(
    x = "Trait 1 centré-réduit"
  ) +
  theme_minimal()

subplot(ggplotly(variance), ggplotly(MSE), shareX = TRUE, shareY = TRUE) |>
  animation_opts(frame = 500) |>
  animation_slider(currentvalue = list(prefix = "Valeur de θ = ", suffix = "°"))
```

On peut s'en convaincre géométriquement et mathématiquement :

![Projection de $\vec{x_{i}}$ sur $\vec{w}$](figures/angle.svg){#fig-proj width="50%"}

On a l'égalité vectorielle :

$$
\text{Résidu} = \vec{x_{i}} - (\vec{x_{i}} \cdot \vec{w})\vec{w}
$$

D'où, en se rappelant que $\|\vec{w}\| = 1$ :

\begin{align}

\| \vec{x_{i}} - (\vec{x_{i}} \cdot \vec{w})\vec{w} \|^{2} &= (\vec{x_{i}} - (\vec{x_{i}} \cdot \vec{w})\vec{w}) \cdot  (\vec{x_{i}} - (\vec{x_{i}} \cdot \vec{w})\vec{w}) \\
                                                           &= \|\vec{x_{i}}\|^{2} - 2(\vec{x_{i}} \cdot \vec{w})^{2} + (\vec{x_{i}} \cdot \vec{w})^2\vec{w} \cdot \vec{w} \\
                                                           &= \|\vec{x_{i}}\|^{2} - (\vec{x_{i}} \cdot \vec{w})^{2}

\end{align}

On a donc la somme des rédisus projetés sur $\vec{w}$ ou Mean Squared Error ($\text{MSE}$) :

\begin{align}

\text{MSE}(\vec{w}) &= \frac{1}{n} \sum_{i = 1}^n(\|\vec{x_{i}}\|^2 - (\vec{x_{i}} \cdot \vec{w})^2) \\
                    &= \frac{1}{n} \sum_{i = 1}^n\|\vec{x_{i}}\|^2 - \frac{1}{n} \sum_{i = 1}^n(\vec{x_{i}} \cdot \vec{w})^2 \\

\end{align}

On veut trouver $\vec{w}$ tel que $\text{MSE}(\vec{w})$ soit minimale. Le premier terme est constant en fonction de $\vec{w}$, il reste donc à maximiser le second terme (il y a un $-$ !).

On remarque que le second terme peut s'écrire $\frac{1}{n} \sum_{i = 1}^n(\vec{x_{i}} \cdot \vec{w})^{2} = \mathbb{E}\left[(\vec{x_{i}} \cdot \vec{w})^{2}\right]$ or on a la relation $\text{Var}(X) = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2}$ d'où l'égalité :

\begin{align}

   \mathbb{E}\left[(\vec{x_{i}} \cdot \vec{w})^{2}\right] &= \mathbb{E}\left[\vec{x_{i}} \cdot \vec{w}\right]^{2} + \text{Var}(\vec{x_{i}} \cdot \vec{w})\\
\frac{1}{n} \sum_{i = 1}^n(\vec{x_{i}} \cdot \vec{w})^{2} &= \left(\frac{1}{n} \sum_{i = 1}^n \vec{x_{i}} \cdot \vec{w}\right)^{2} + \text{Var}(\vec{x_{i}} \cdot \vec{w}) \\
                                                          &= \text{Var}(\vec{x_{i}} \cdot \vec{w})

\end{align}

Car $\sum_{i = 1}^n \vec{x_{i}} \cdot \vec{w} = (\sum_{i = 1}^n\vec{x_{i}}) \cdot \vec{w} = 0$ puisque les $X_{p}$ sont des variables centrées de moyennes nulles donc $\frac{1}{n} \sum_{i = 1}^n \|\vec{x_{i}}\| = 0$.

Ainsi on a :

$$
\text{MSE}(\vec{w}) = \frac{1}{n}\sum_{i = 1}^n \|\vec{x_{i}}\|^2 - \text{Var}(\vec{x_{i}} \cdot \vec{w})
$$

Minimiser $\text{MSE}(\vec{w})$ revient donc bien à maximiser la variance des projections des $\vec{x_{i}}$ mais comment faire ?

En rappelant :

$$
X \vec{w} = \begin{bmatrix} x_{1,1} & \dots  & x_{1,p} \\
                                 \vdots & \ddots & \vdots \\
                                x_{n,1} & \dots  & x_{n,p}  \end{bmatrix} \times \begin{bmatrix} w_{1} \\ 
                                                                                                 \vdots \\
                                                                                                 w_{p} \end{bmatrix} = \begin{bmatrix} \sum_{i = 1}^{p} x_{i, 1}.w_{i}\\
                 \vdots \\
                 \sum_{i = 1}^{p} x_{i, n}.w_{i} \end{bmatrix} =
\begin{bmatrix} \vec{x_{1}} \cdot \vec{w} \\
                            \vdots \\
                \vec{x_{2}} \cdot \vec{w} \end{bmatrix}
$$

et

$$
(X\vec{w})^{T}(X\vec{w}) = \sum_{i = 1}^{n} (\vec{x_{i}} \cdot \vec{w})^{2}
$$

On obtient :

\begin{align}

\sigma_{\vec{w}}^{2} = \text{Var}(\vec{x_{i}} \cdot \vec{w}) &= \frac{1}{n} \sum_{i = 1}^{n}(\vec{x_{i}} \cdot \vec{w})^{2} \\
                                                             &= \frac{1}{n}(X\vec{w})^{T}(X\vec{w}) \\
                                                             &= \frac{1}{n} \vec{w}^{T} X^{T} X \vec{w} \\
                                                             &= \vec{w}^{T} \frac{X^{T} X}{n} \vec{w} \\
                                        \sigma_{\vec{w}}^{2} &= \vec{w}^{T} cov_{X^{T}, X} \vec{w}

\end{align}

avec $cov_{X^T,X} \in \mathcal{M}_{p}(\mathbb{R})$ la matrice de variance-covariance des $X_{p}$ variables aléatoires de dimension $p \times p$ correspondants à nos paramètres :

\begin{align}

cov_{X^{T}, X} = \text{Cov}\left[X^{T}, X\right] &= \mathbb{E}\left[(X - \mu_{X})^{T}(X - \mu_{X})\right] \\
                                                 &= \mathbb{E}\left[X^{T}X\right] \text{ en centrant les } X_{p}\\
\end{align}

::: {.callout-note}

En développant :

\begin{align}
cov_{X^{T}, X} &= \begin{bmatrix} \text{Var}(X_{1}) & \dots  & \text{Cov}(X_{1}, X_{p}) \\
                                             \vdots & \ddots & \vdots \\
                           \text{Cov}(X_{p}, X_{1}) & \dots  & \text{Var}(X_{p}) \end{bmatrix} =
\begin{bmatrix} \frac{1}{n}\sum_{i=1}^{n}x_{i, 1}^{2} & \dots  &  \frac{1}{n}\sum_{i=1}^{n}x_{i, 1}x_{i, p} \\
                                               \vdots & \ddots & \vdots \\
            \frac{1}{n}\sum_{i=1}^{n}x_{i, p}x_{i, 1} & \dots  & \frac{1}{n}\sum_{i=1}^{n}x_{i, p}^{2} \end{bmatrix} \\
               &= \frac{1}{n} \begin{bmatrix} x_{1, 1} & \dots  & x_{n, 1} \\
                                                \vdots & \ddots & \vdots \\
                                              x_{1, p} & \dots  & x_{n, p} \end{bmatrix} \times \begin{bmatrix} x_{1, 1} & \dots  & x_{1, p}  \\
                                                                                                                  \vdots & \ddots & \vdots \\
                                                                                                                x_{n, 1} & \dots  & x_{n, p} \end{bmatrix} = \frac{1}{n} X^{T}X
\end{align}

:::

### Un problème d'optimisation

Ainsi, minimiser $\text{MSE}(\vec{w})$ revient à trouver $\vec{w}$, vecteur unitaire, tel que $\sigma_{\vec{w}}^2$ soit minimisé.

Pour se faire, on peut utiliser le multiplicateur de Lagrange $\lambda \in \mathbb{R}$, qui permet de poser mathématiquement des problèmes d'optimisation, avec la contrainte que $\vec{w}$ doit être un vecteur unitaire, c'est à dire que $\|\vec{w}\| = \vec{w}^T\vec{w} = 1$ comme suit :

\begin{align}

               \mathcal{L}(\vec{w}, \lambda) &= \sigma_{\vec{w}}^2 - \lambda(\vec{w}^T\vec{w} - 1) \\
&\begin{cases}
\frac{\partial\mathcal{L}}{\partial \lambda} =      (\vec{w}^T\vec{w} - 1) \\
      \frac{\partial\mathcal{L}}{\partial w} =      2cov_{X^T,X}\vec{w} - 2\lambda \vec{w}
\end{cases}
\end{align}

On a donc à l'optimum, en posant les dérivées partielles nulles, le système d'équations :

\begin{cases}

 \vec{w}^T \vec{w} &= 1 \\
cov_{X^T,X}\vec{w} &= \lambda \vec{w}

\end{cases}

Ce qui revient à trouver les valeurs et vecteurs propres de $cov_{X^T,X}$ !

On a aussi :

\begin{align}

cov_{X^T,X}\vec{w} = \lambda \vec{w} &\Rightarrow \vec{w}^Tcov_{X^T,X}\vec{w} = \vec{w}^{T}\lambda \vec{w} = \|\vec{w}\|^{2}\lambda \\
                                     &\Rightarrow \sigma_{\vec{w}}^{2}= \lambda \\

\end{align}

Donc le vecteur propre associé à la valeur propre la plus élevée maximisera la covariance projetée sur $\vec{w}$.

Et ça nous arrange bien car la matrice de covariance a des propriétés intéressantes : c'est une matrice carrée, positive, symétrique et inversible. Donc il existe une décomposition en valeurs propres de cette matrice telle que $cov_{X^T,X}$ se factorise sous la forme :

$$
cov_{X^{T},X} = X^{T}X = W \Lambda W^{-1}
$$

Avec $W \in \mathcal{M}_{p}(\mathbb{R})$ la matrice des vecteurs propres (tous les $\vec{w}$ !) et $\Lambda \in \mathcal{M}_{p}(\mathbb{R})$ la matrice diagonale des valeurs propres de $cov_{X^T,X}$.\
Les vecteurs propres ainsi trouvés sont tous unitaires et orthogonaux et forment une base de l'espace à $p$ dimensions de départ.

On a alors les coordonnées de notre nuage de points initial dans le nouveau repère défini par $W$ qui sont données par :

$$
T = XW
$$

On appelle $T \in \mathcal{M}_{n,p}(\mathbb{R})$ matrice des scores.\
Et la matrice $W$ des vecteurs propres contient les poids ("loadings"), c'est-à-dire les combinaisons linéaires des paramètres initiaux qui donnent les vecteurs propres (leurs coordonnées dans le référentiel des vecteurs propres).

### La décomposition en valeur singulières

En pratique, Le calcul de $cov_{X^T,X}$ est gourmand. Une "astuce" informatique consiste à réaliser une décomposition en valeurs singulières de $X$ tel que :

$$
X = U \Sigma V^T
$$

avec $U$ une matrice unitaire (càd $UU^T = I$) de dimension $n \times n$, $V$ une matrice unitaire de dimension $p \times p$ et $\Sigma$ une matrice diagonale rectangulaire de dimension $n \times p$ dont les coefficients diagonaux sont les $\sqrt{\lambda_{i}}$ non nulles.

![Interprétation graphique de la Décomposition en Valeurs Singulières](figures/Singular-Value-Decomposition.svg){#fig-svd width="50%"}

On a alors la relation :

$$
X^TX = V \Sigma^T U^T U \Sigma V^T = V (\Sigma^T\Sigma) V^T =  W \Lambda W^{-1}
$$

On retrouve la décomposition en valeurs propres de la matrice de variance-covariance. En décomposant $X$ en valeurs singulières, on s'évite le calcul explicite de $cov_{X^T,X}$ !

## Exemple simple à trois dimensions

```{r}
library(FactoMineR)
```

```{r}
#| echo: false
#| label: tbl-iris
#| tbl-cap: "Extrait des valeurs des données Iris"

data("iris")

iris_3D <- iris |> 
  select(c(Species, Sepal.Width, Petal.Width, Petal.Length))

kable(head(iris_3D))
```

```{r}
#| echo: false
#| label: fig-iris
#| fig-cap: Plots des valeurs de Iris

iris |> plot_ly(
  x      = ~Sepal.Width,
  y      = ~Petal.Width,
  z      = ~Petal.Length,
  color  = ~Species,
  colors = c('#BF382A', '#0C4B8E', "green")
) |> 
  add_markers() |> 
  layout(
    scene = list(
      xaxis      = list(title = "Sepal Width"),
      yaxis      = list(title = "Petal Width"),
      zaxis      = list(title = "Petal Length"),
      aspectmode = 'data',
      camera     = list(projection = list(type = "orthographic"))
    )
  )
```

```{r}
#| echo: false
#| label: fig-irisscaled
#| fig-cap: Plots des valeurs de Iris centrées et réduites

iris |> 
  mutate(
    Sepal.Length = scale(Sepal.Length),
    Sepal.Width  = scale(Sepal.Width),
    Petal.Length = scale(Petal.Length),
    Petal.Width  = scale(Petal.Width)
    ) |> 
  plot_ly(
    x      = ~Sepal.Width,
    y      = ~Petal.Width,
    z      = ~Petal.Length,
    color  = ~Species,
    colors = c('#BF382A', '#0C4B8E', "green")
  ) |> 
  add_markers() |> 
  layout(
    scene = list(
      xaxis      = list(title = "Sepal Width"),
      yaxis      = list(title = "Petal Width"),
      zaxis      = list(title = "Petal Length"),
      aspectmode = 'data',
      camera     = list(projection = list(type = "orthographic"))
    )
  )
```

```{r}
#| echo: false
#| label: fig-irispca
#| fig-cap: Plots des valeurs de l'ACP de Iris

iris |>
  select(-c(Sepal.Length, Species)) |> 
  PCA(graph = FALSE) |> 
  _$ind |> 
  _$coord |>
  as_tibble() |>
  mutate(Species = iris$Species) |> 
  plot_ly(
    x      = ~Dim.1,
    y      = ~Dim.2,
    z      = ~Dim.3,
    color  = ~Species,
    colors = c('#BF382A', '#0C4B8E', "green")
  ) |> 
  add_markers() |> 
  layout(
    scene = list(
      domain     = list(x = c(-3, 3), y = c(-3, 3), z = c(-3, 3)),
      xaxis      = list(title = "PC 1"),
      yaxis      = list(title = "PC 2"),
      zaxis      = list(title = "PC 3"),
      aspectmode = 'data',
      camera     = list(projection = list(type = "orthographic"))
    )
  )
```

## Et en pratique ?

```{r}
library(factoextra)
library(patchwork)
```

```{r}
iris_pca <- iris |>
  select(-c(Species)) |>
  PCA(
    scale.unit = TRUE,
    ncp        = 5,
    graph      = FALSE
  )

iris_pca
```

```{r}
iris |>
  select(-c(Species)) |>
  prcomp(scale. = TRUE)
```

[Aide](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp) de la fonction `prcomp()`.

On commence par deux types de graphiques : Un Screeplot et un cercle des corrélations.

```{r}
#| echo: true
#| label: fig-screecirc
#| fig-cap: Screeplot et cercle des corrélations pour Iris

(fviz_eig(iris_pca, addlabels = TRUE) +
 coord_cartesian(ylim = c(0, 100))) |
  (fviz_pca_var(iris_pca, col.var = "black") +
   coord_cartesian(ratio = 1, xlim = c(-1.25, 1.25), ylim = c(-1.25, 1.25)) +
   theme_classic())
```

La fonction `fviz_pca_var()` permet de colorer les vecteurs des variables par le carré de leur cosinus avec les vecteurs propres :

```{r}
iris_pca |>
  fviz_pca_var(
    col.var       = "cos2",
    gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
    repel         = TRUE # Avoid text overlapping
  ) +
  theme_classic()
```

Une autre manière de visualiser ces contributions :

```{r}
library(corrplot)

var <- get_pca_var(iris_pca)
corrplot(var$cos2, is.corr = FALSE)

(fviz_contrib(iris_pca,
             choice = "var",
             axes   = 1,
             top    = 10) +
  coord_cartesian(ylim = c(0, 100))) |
(fviz_contrib(iris_pca,
             choice = "var",
             axes   = 2,
             top    = 10) +
   coord_cartesian(ylim = c(0, 100)))
```

On peut maintenant regarder ce qu'il se passe du côté des individus :

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(1, 2),
  geom.ind   = "point",
  mean.point = FALSE
)
```

Il semble y avoir plusieurs groupes, ajoutons une variable catégorielle pour aider à la visualisation :

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(1, 2),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)
```

Les axes 1 et 2 de l'ACP nous permettent de distinguer clairement deux groupes qui correspondent bien à des espèces différentes.

```{r}
iris_pca |> fviz_pca_ind(
  axes       = c(2, 3),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)

iris_pca |> fviz_pca_ind(
  axes       = c(3, 4),
  geom.ind   = "point",
  col.ind    = iris$Species, 
  palette    = c("#00AFBB", "#E7B800", "#FC4E07"),
  mean.point = FALSE
)
```

Par contre les axes 2 et 3 sont moins informatifs dans ce cas, comme prédis par le screeplot.

## À retenir de l'ACP

-   C'est une analyse exploratoire d'un jeu de données *a priori* corrélées, pas un test statistique. Elle permet de décrire la variance d'un jeu de données

-   Cette analyse permet de réduire la dimensionnalité des données et les résume par des composantes principales

-   Ces composantes principales sont orthogonales (*i. e* non corrélées) et participent de manière décroissante à la variance totale du jeu de données

-   Le nouvel espace est Euclidien, on peut y mesurer des distances et des angles. Attention cependant à leur interprétation !

## Analyses de la même famille

WORK IN PROGRESS

<!-- Source : <http://factominer.free.fr/factomethods/index_fr.html> -->

<!-- ### Méthodes Classiques -->

<!-- Quand des individus sont décrits par un jeu de variables, plusieurs méthodes sont possibles selon le type de variables considéré (quantitatives ou qualitatives): -->

<!-- #### Ordination sans contraintes -->

<!-- -   Quand les variables sont quantitatives, on peut réaliser une Analyse en Composantes Principales (ACP ou PCA). On a alors des *distances Euclidiennes* entre individus -->

<!-- -     Analyse en *Coordonnées* Principales (ACoP ou PCoA) si on veut utiliser une distance encore différente. C'est la version la plus générale de ces analyses -->

<!-- #### Ordination sous contraintes -->

<!-- -   Quand les individus sont décrits par deux variables qualitatives, on peut construire un tableau de contingence et réaliser une Analyse Factorielle des Correspondances (AFC ou CA). L’une des hypothèses clefs de la PCA postule que les individus sont liés façon linéaire. Quand ce n'est pas le cas on peut avoir des artéfacts, on peut alors utiliser une distance du Chi2 -->

<!-- -   Quand les individus sont décrits par un jeu de variables qualitatives, on peut réaliser une Analyse des Correspondances Multiples (ACM ou MCA). -->

<!-- La fonction HCPC (Classification Hiérarchique sur Composantes principales) permet de réaliser une classification non supervisée des individus. Cette fonction combine les facteurs principaux, la classification hérarchique et le partitionnement pour mieux visualiser et mettre l'accent sur les similarités entre individus. -->

<!-- ### Méthodes Avancées -->

<!-- #### Un groupe d'individus et plusieurs groupes de variables -->

<!-- Quand des individus sont décrits par plusieurs groupes de variables, plusieurs types d'analyses sont proposées: -->

<!-- -   AFM (Analyse Factorielle Multiple), où les variables d'un même groupe peuvent être quantitatives ou qualitatives -->

<!-- -   AFMH (Analyse Factorielle Multiple Hiérarchique), une extension de l'AFM où les variables sont structurées selon une hiérarchie. -->

<!-- -   GPA (Analyse Procustéenne Généralisée), où les variables doivent être quantitatives. -->

<!-- #### Un groupe de variables, plusieurs groupes d'individus -->

<!-- Quand plusieurs groupes d'individus sont décrits par un groupe de variables quantitatives, l'analyse que l'on propose est une extension de l'AFM appelée AFM Duale. -->

<!-- #### Un groupe d'individus, deux types de variables -->

<!-- Quand un groupe d'individus est décrit par un groupe de variables quantitatives et/ou qualitatives, on propose comme analyse un cas particulier de l'AFM appelé Analyse Factorielle de Données Mixtes. -->

<!-- ### Analyses discriminantes -->
